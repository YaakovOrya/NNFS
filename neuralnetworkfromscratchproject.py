# -*- coding: utf-8 -*-
"""NeuralNetworkFromScratchProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g4iYVekthFnLntnuIyWFHFkw-iyZ-H2l
"""

import random
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

"""#Custom Multiclass Neural Network Classifier: A TensorFlow-Free Approach.

## Dataset

In gradient descent optimization for machine learning models, feature scaling is important because it ensures that the updates to model weights are balanced across different features.

Without scaling, features with larger scales can dominate the optimization process, leading to slow convergence, overshooting, or oscillation.

Feature scaling brings all features to a similar scale, promoting numerical stability, faster convergence, and a more reliable optimization process.
"""

path1='/content/drive/MyDrive/neural network from scratch /train_X.csv'
path2='/content/drive/MyDrive/neural network from scratch /train_label.csv'
path3='/content/drive/MyDrive/neural network from scratch /test_X.csv'
path4='/content/drive/MyDrive/neural network from scratch /test_label.csv'

# transposes the loaded data, swapping rows and columns.
X_train = np.loadtxt(path1, delimiter = ',').T
y_train = np.loadtxt(path2, delimiter = ',')


X_test = np.loadtxt(path3, delimiter = ',').T
y_test = np.loadtxt(path4, delimiter = ',').T

# Split the data into training and cross-validation sets
X_train, X_cv, y_train, y_cv = train_test_split(X_train.T, y_train, test_size=300, random_state=42)

# Transpose the data back to the original shape(columns, rows)
X_train = X_train.T
X_cv = X_cv.T
y_train = y_train.T
y_cv = y_cv.T

# Print the shapes of the resulting sets
print("Training set shapes:")
print("X_train:", X_train.shape)
print("y_train:", y_train.shape)

print("\nCross-validation set shapes:")
print("X_cv:", X_cv.shape)
print("y_cv:", y_cv.shape)

# Remaining shapes
print("\nRemaining test set shapes:")
print("X_test:", X_test.shape)
print("y_test:", y_test.shape)

# Continue with the rest of your model code...

#This line generates a random index within the range of the number of examples in your training dataset (X_train). It randomly selects one example from your dataset.

index = random.randrange(0, X_train.shape[1])

#This extracts the column at the randomly selected index, representing the flattened image.

plt.imshow(X_train[:, index].reshape(28,28),cmap='gray')

# Displays the reshaped image using Matplotlib's imshow function.
plt.show()

"""In the context of a flattened representation of images where each image is represented as a one-dimensional vector, the convention [num_features,          num_examples] or [columns, rows] would mean:   

Each column represents an example (a flattened image)
Each row represents a different feature in that example.
So, if you have a dataset of flattened images, and you organize them in a matrix where each column corresponds to a flattened image, and each row corresponds to a feature (pixel in this case), then the shape of the matrix would be [num_features, num_examples] or [columns, rows].

##Activation Functions
"""

def sigmoid(Z):
    A = 1/(1+np.exp(-Z))
    return A

def softmax(z):
    expZ = np.exp(z)
    return expZ/(np.sum(expZ, 0))

def relu(Z):
    A = np.maximum(0,Z)
    return A

def tanh(x):
    return np.tanh(x)

def derivative_relu(Z):
    return np.array(Z > 0, dtype = 'float')

def derivative_tanh(x):
    return (1 - np.power(x, 2))

"""##Initialize Parameters

####The function initializes the parameters of the neural network, including weights (W) and biases (b), for each layer.
###Loop through Layers:
The function loops through each layer starting from the second layer (index 1) because the input layer (layer 0) does not have weights and biases.
###Weight Initialization (He Initialization):
For each layer, it initializes the weight matrix W with random values drawn from a normal distribution.
The scaling factor np.sqrt(layer_dims[l-1]) is used for He initialization. This scaling helps in keeping the variance of activations roughly constant across different layers.

"He initialization" refers to a specific method for initializing the weights of a neural network, and it is named after the researcher Kaiming He who introduced this technique. The goal of He initialization is to address the vanishing/exploding gradient problem that can occur during the training of deep neural networks.

### why we scale the parameters?
#####Vanishing Gradients with Large Initialization:

When parameters are initialized with very large values (e.g., large weights), and activation functions like sigmoid or tanh are used, the gradients during backpropagation can become very small.
The vanishing gradients problem occurs when the derivatives of the activation functions with respect to their inputs approach zero for large inputs. This results in small gradient values during backpropagation.
Small gradients lead to tiny updates to the parameters during optimization, making it challenging for the model to learn effectively, and causing slow convergence.

#####Exploding Gradients with Very Large Initialization:

On the other hand, when parameters are initialized with very large values, the gradients during backpropagation can become very large.
Exploding gradients occur when the gradients become extremely large, causing large updates to the parameters during optimization.
Large updates can lead to oscillations or divergence of the optimization algorithm, making it difficult to find the optimal set of parameters.

#####Smaller Updates for Smaller Initial Values:

When parameters are initialized with smaller values, the gradients during backpropagation are generally larger. As a result, the updates to the parameters are also larger.
These larger updates allow the optimization algorithm to make more significant progress in the parameter space, helping it converge to the optimal solution more quickly.

###Bias Initialization:
It initializes the bias vector b with zeros.
###Return:
The function returns the initialized parameters as a dictionary (parameters), where keys are strings representing the layer type ('W' for weights and 'b' for biases) and the layer index.
"""

def initialize_parameters(layer_dims):


    parameters = {}

    L = len(layer_dims)

    for l in range(1, L):

        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])

        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))

    return parameters

# A quit Check if our functions is right.
layer_dims = [X_train.shape[0], 100, 200, y_train.shape[0]]
params = initialize_parameters(layer_dims)

for l in range(1, len(layer_dims)):
    print("Shape of W" + str(l) + ":", params['W' + str(l)].shape)
    print("Shape of B" + str(l) + ":", params['b' + str(l)].shape, "\n")

"""## Forward Propagation

####Initialization: The function initializes forward_cache, a dictionary used to store intermediate values during forward propagation.

####Input Activation: The input data X is set as the initial activation A0 in the forward_cache.

####Hidden Layers Loop: The function iterates through hidden layers (from 1 to L-1) and performs the following steps for each layer:

#####a. Compute the linear transformation Z for the current layer using the weights (W) and biases (b).

#####b. Apply the specified activation function (tanh or relu) to compute the activation A for the current layer.

####Output Layer Computation: The function computes the linear transformation and activation for the output layer (layer L) based on the specified activation function (sigmoid or softmax).

####Return: The function returns the final output for the output layer (A + str(L)) and the dictionary forward_cache containing intermediate values for each layer.
"""

def forward_propagation(X, parameters, activation):

    forward_cache = {}
    L = len(parameters) // 2

    forward_cache['A0'] = X

    for l in range(1, L):
        forward_cache['Z' + str(l)] = parameters['W' + str(l)].dot(forward_cache['A' + str(l-1)]) + parameters['b' + str(l)]

        if activation == 'tanh':
            forward_cache['A' + str(l)] = tanh(forward_cache['Z' + str(l)])
        else:
            forward_cache['A' + str(l)] = relu(forward_cache['Z' + str(l)])


    forward_cache['Z' + str(L)] = parameters['W' + str(L)].dot(forward_cache['A' + str(L-1)]) + parameters['b' + str(L)]

    if forward_cache['Z' + str(L)].shape[0] == 1:
        forward_cache['A' + str(L)] = sigmoid(forward_cache['Z' + str(L)])
    else :
        forward_cache['A' + str(L)] = softmax(forward_cache['Z' + str(L)])

    return forward_cache['A' + str(L)], forward_cache

aL, forw_cache = forward_propagation(X_train, params, 'relu')

for l in range(len(params)//2 + 1):
    print("Shape of A" + str(l) + " :", forw_cache['A' + str(l)].shape)

"""##Cost Function

####The function calculates the original cost based on the type of output layer (binary or multi-class).
#####For binary classification (Y.shape[0] == 1), it uses the binary cross-entropy loss formula.
#####For multi-class classification, it uses the categorical cross-entropy loss formula.

###Regularization Term Calculation:
It computes the regularization term for the cost function. This term is added to the original cost to penalize large weights and prevent overfitting.
The regularization term is the sum of the squared values of all weights in the neural network.

### Why we dont regularize the bias term?

Here σ(⋅) is the sigmoid function. If b is very large (positive or negative), it will shift the entire argument of the sigmoid function, influencing where the sigmoid function starts transitioning from near zero to one. This shift is what's meant by controlling the position or baseline of the activation function.

If regularization were applied to the bias term, it could penalize large values of b, potentially restricting the ability of the model to shift and adapt to the data. This is why, in practice, bias terms are often left unregularized, allowing the model the flexibility it needs to learn the appropriate shifts in the data space.


###Regularization Term Addition:
The regularization term is added to the original cost with a scaling factor (lambda_val / (2 * m)).
###Squeezing Cost:
The np.squeeze(cost) is used to remove single-dimensional entries from the shape of an array. It ensures that the cost has the expected shape.
###Return:

The function returns the final cost.
This cost function includes both the original cost (based on the type of output layer),
and a regularization term to balance the model's fit to the training data and prevent overfitting.
The regularization parameter lambda_val controls the strength of regularization.
"""

def compute_cost(AL, Y, parameters, lambda_val):
    m = Y.shape[1]

    if Y.shape[0] == 1:
        original_cost = (1/m) * (-np.dot(Y, np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))
    else:
        original_cost = -(1./m) * np.sum(Y * np.log(AL))

    regularization_term = 0
    L = len(parameters) // 2

    for l in range(1, L + 1):
        W = parameters['W' + str(l)]
        regularization_term += np.sum(np.square(W))

    cost = original_cost + (lambda_val / (2 * m)) * regularization_term

    cost = np.squeeze(cost)

    return cost

"""##Back Propagation

###Applying the chain rule of calculus

###Initialization:
grads: A dictionary to store the gradients of the cost with respect to parameters.
###Output Layer Gradients (Layer L):
Compute the gradient of the cost with respect to the output (AL) and store it in grads["dZ" + str(L)].
Compute gradients for weights and biases of the output layer.
###Hidden Layer Gradients (from L-1 to 1):
Iterate through hidden layers in reverse order.
Compute the gradient of the activation for hidden layers (grads["dZ" + str(l)]) using the chain rule.
Compute gradients for weights and biases of hidden layers.
###Activation Function Derivatives:
The code uses either the tanh or ReLU activation function based on the activation parameter. It calls derivative_tanh or derivative_relu to compute the derivative of the activation function.
###Regularization Term in Gradients:
The regularization term is added to the gradients for the weight matrices (grads["dW" + str(l)]) to prevent overfitting.
###Return:
The function returns the dictionary grads containing the computed gradients.
"""

def backward_propagation(AL, Y, parameters, forward_cache, activation, lambda_val=1):
    grads = {}
    L = len(parameters) // 2
    m = AL.shape[1]

    grads["dZ" + str(L)] = AL - Y
    grads["dW" + str(L)] = (1. / m) * np.dot(grads["dZ" + str(L)], forward_cache['A' + str(L - 1)].T) + (lambda_val / m) * parameters['W' + str(L)]
    grads["db" + str(L)] = (1. / m) * np.sum(grads["dZ" + str(L)], axis=1, keepdims=True)

    for l in reversed(range(1, L)):
        if activation == 'tanh':
            grads["dZ" + str(l)] = np.dot(parameters['W' + str(l + 1)].T, grads["dZ" + str(l + 1)]) * derivative_tanh(forward_cache['A' + str(l)])
        else:
            grads["dZ" + str(l)] = np.dot(parameters['W' + str(l + 1)].T, grads["dZ" + str(l + 1)]) * derivative_relu(forward_cache['A' + str(l)])

        grads["dW" + str(l)] = (1. / m) * np.dot(grads["dZ" + str(l)], forward_cache['A' + str(l - 1)].T) + (lambda_val / m) * parameters['W' + str(l)]
        grads["db" + str(l)] = (1. / m) * np.sum(grads["dZ" + str(l)], axis=1, keepdims=True)

    return grads

grads = backward_propagation(forw_cache["A" + str(3)], y_train, params, forw_cache, 'relu',lambda_val=1)

for l in reversed(range(1, len(grads)//3 + 1)):
    print("Shape of dZ" + str(l) + " :", grads['dZ' + str(l)].shape)
    print("Shape of dW" + str(l) + " :", grads['dW' + str(l)].shape)
    print("Shape of dB" + str(l) + " :", grads['db' + str(l)].shape, "\n")

"""##Update Parameters

###Number of Layers (L):
L = len(parameters) // 2: This calculates the number of layers in the neural network. It assumes that the parameters dictionary contains keys in the form of "W1", "b1", "W2", "b2", ..., "WL", "bL", where "Wl" and "bl" represent the weight matrix and bias vector for layer l.
###Gradient Descent Update:
The function uses a loop to iterate through each layer of the neural network.
For each layer, it updates the weights and biases using the gradient descent update rule:
parameter
=
parameter
−
learning_rate
×
gradient
The parameters here refer to both weights (W) and biases (b).

###Updating Weights (Wl) and Biases (bl):
parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * grads["dW" + str(l)]: Update the weight matrix for layer l.

parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * grads["db" + str(l)]: Update the bias vector for layer l.

###Return Updated Parameters:
The function returns the updated parameters after completing the updates for all layers.
"""

def update_parameters(parameters,grads,learning_rate):

  L = len(parameters)//2

  for l in range (1,L+1):
    parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * grads["dW" + str(l)]
    parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * grads["db" + str(l)]

  return parameters

"""##MODEL

###Input Parameters:
X: Input data matrix with features as rows.

y: True labels.

parameters: Dictionary containing the learned weights and biases of the neural network.

activation: Activation function used in the hidden layers.
###Compute Predictions:
The function uses the forward_propagation function to compute the predicted values y_pred and caches from the forward pass of the neural network.
###Adjust for Binary or Multiclass Classification:
If there is only one output node (binary classification), the predicted probabilities are converted to binary predictions using a threshold of 0.5.
If there are multiple output nodes (multiclass classification), both true labels (y) and predictions (y_pred) are converted to their argmax values.
###Compute Accuracy:
The accuracy is calculated by comparing the rounded predictions (y_pred) with the true labels (y).
The sum of correct predictions is divided by the total number of examples (m).
The result is rounded to two decimal places using np.round.
###Return Accuracy:
The final accuracy value is returned by the function.
"""

def accuracy(X, y, parameters, activation):

    m = X.shape[1]
    y_pred, caches = forward_propagation(X, parameters, activation)

    if y.shape[0] == 1:
        y_pred = np.array(y_pred > 0.5, dtype = 'float')
    else:
        y = np.argmax(y, 0)
        y_pred = np.argmax(y_pred, 0)

    return np.round(np.sum((y_pred == y)/m), 2)

"""This code defines a function model that trains a neural network with mini-batch gradient descent. It uses a specified activation function, and optionally applies L2 regularization. The training progress is monitored, and the function also provides visualizations of the training loss, accuracy, and the evolution of the weights of the first layer over epochs.

###Initialization:
Random initialization of parameters (weights and biases) is performed using initialize_parameters function.
The initial values of the first layer weights are stored.
###Training Loop:
The function iterates through a specified number of epochs.
In each epoch, the training data is shuffled, and mini-batch gradient descent is performed.
The cost, gradients, and parameters are updated for each mini-batch.
###Cost and Accuracy Tracking:
The training cost is stored in the costs list for each epoch.
Training accuracy and cross-validation accuracy are computed periodically during training and stored in train_accuracies and cv_accuracies lists, respectively.
###Print Progress:
Training progress is printed every 10% of the total epochs, including the iteration number, cost, training accuracy, and cross-validation accuracy.
A visual indicator ("==>") is printed every 10 iterations.
###Plotting:
The training loss, training accuracy, and cross-validation accuracy are plotted over epochs using matplotlib.
The evolution of the first layer weights is also plotted to observe how they change during training.
###Return:
The trained parameters are returned after training.

"""

def model(X, Y,X_cv,y_cv, layers_dims, learning_rate=0.03, lambda_val=None, activation='relu', epochs=3000, minibatch_size=None):

    costs = []
    train_accuracies = []  # Store training accuracies
    cv_accuracies = []     # Store cross-validation accuracies

    parameters = initialize_parameters(layers_dims)

    # Store the initial values of the first layer weights
    initial_weights = parameters['W1'][0, 0]

    # Lists to store the evolution of the first layer weights
    weights_over_epochs = [initial_weights]

    num_samples = X.shape[1]  # Number of examples is now the number of columns

    for i in range(0, epochs):
        # Shuffle the training data and labels in each epoch
        permutation = np.random.permutation(num_samples)
        X_shuffled = X[:, permutation]
        Y_shuffled = Y[:, permutation]

        for j in range(0, num_samples, minibatch_size):
            # Get mini-batch
            end_idx = min(j + minibatch_size, num_samples)
            X_mini_batch = X_shuffled[:, j:end_idx]
            Y_mini_batch = Y_shuffled[:, j:end_idx]

            AL, forward_cache = forward_propagation(X_mini_batch, parameters, activation)
            cost = compute_cost(AL, Y_mini_batch, parameters, lambda_val)
            grads = backward_propagation(AL, Y_mini_batch, parameters, forward_cache, activation, lambda_val)
            parameters = update_parameters(parameters, grads, learning_rate)

        # Store the training loss at each iteration
        costs.append(cost)

        # Store the evolution of the first layer weights at each iteration
        weights_over_epochs.append(parameters['W1'][0, 0])

        if i % (epochs // 10) == 0:
            train_acc = accuracy(X, Y, parameters, activation)
            cv_acc = accuracy(X_cv, y_cv, parameters, activation)
            train_accuracies.append(train_acc)
            cv_accuracies.append(cv_acc)
            print("\niter:{} \t cost: {} \t train_acc:{} \t cv_acc:{}".format(i, np.round(cost, 2), train_acc, cv_acc))

        if i % 10 == 0 :
            print("==>", end='')




    # Plotting the training loss in purple
    plt.plot(range(epochs), costs, label='Training Cost', color='purple')
    plt.xlabel('Epochs')
    plt.ylabel('Cost')
    plt.title('Training Cost Over Epochs')
    plt.legend()
    plt.show()

    # Plotting the accuracies
    plt.plot(range(0, epochs, epochs // 10), train_accuracies, label='Training Accuracy')
    plt.plot(range(0, epochs, epochs // 10), cv_accuracies, label='Cross-Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training and Cross-Validation Accuracies')
    plt.legend()
    plt.show()

    # Plotting the evolution of the first layer weights in black
    plt.plot(range(epochs + 1), weights_over_epochs, label='First Layer Weights', color='black')
    plt.xlabel('Epochs')
    plt.ylabel('Weight Value')
    plt.title('Evolution of First Layer Weights Over Epochs')
    plt.legend()
    plt.show()

    return parameters

"""###Network Architecture:
The neural network architecture is defined by layer_dims. It has an input layer with the number of features in X_train, two hidden layers with 128 and 60 units, a third hidden layer with 30 units, and an output layer with the number of classes in y_train.
###Hyperparameters:
lv: L2 regularization parameter.
alpha: Learning rate for gradient descent.
iters: Number of training epochs.
batch_size: Batch size for mini-batch gradient descent. It is set to the number of examples in the training set (X_train.shape[1]).
###Training the Model:
The model function is called with the training data (X_train, y_train), cross-validation data (X_cv, y_cv), and other specified parameters.
The function returns the trained parameters.
###Print Test Accuracy:
After training, the accuracy of the trained model is evaluated on the test set (X_test, y_test) using the accuracy function.
The test accuracy is then printed.
"""

layer_dims = [X_train.shape[0],128,128,60,15,y_train.shape[0]]
lv =500
alpha = 0.001
iters = 1600
batch_size = X_train.shape[1]
#X_train.shape[1]

parameters = model(X_train, y_train,X_cv,y_cv, layer_dims, learning_rate=alpha, lambda_val=lv, activation='relu', epochs=iters, minibatch_size = batch_size)
print('\n')
test_accuracy = accuracy(X_test, y_test, parameters, activation='relu')

print(f"Test accuracy: {test_accuracy} ")

